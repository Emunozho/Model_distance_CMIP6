{\rtf1\ansi\deff0{\fonttbl{\f0 \fswiss Helvetica;}{\f1 \fmodern Courier;}}
{\colortbl;\red255\green0\blue0;\red0\green0\blue255;}
\widowctrl\hyphauto

{\pard \ql \f0 \sa180 \li0 \fi0 {\i This is a nice study that proposes an information-theoretic rationale for weighting ESM outputs when computing multi-model average projections. The approach constructs weights from the divergence between each ESM\u8217's output distribution and the observed-climate distribution, thereby rewarding models that align more closely with an observational product. The method is demonstrated on an ensemble of eight CMIP6 models to project net ecosystem exchange of CO{\sub 2} and net biome production, with weighting schemes calibrated against observational datasets. I found the study well written, with a clear and intuitive presentation of the information-theoretic background. These concepts are often missing from discussions of climate-model post-processing, and it is refreshing to see them used here. I also enjoyed learning about the connection between cross-entropy and AIC. I have a small quibble with calling the KL divergence a distance, but I will not press the point because the term likely helps build intuition.}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We thank the reviewer for the positive and constructive evaluation of our article. We agree with the reviewer\u8217's concern about our use of the term \u8216'distance\u8217', and acknowledge that \u8216'divergence\u8217' is a more appropriate term. Most introductory textbooks on information theory make this clarification, but we used the word in our original version assuming no previous knowledge of readers on the details of distance metrics (norms) in mathematics. However, we see now the potential source of confusion given that a portion of the readers of this article might be familiar with the mathematical definition of metric and norm.\line To address this issue, we added in section 2 a paragraph clarifying the difference between \u8216'distance\u8217' and \u8216'divergence\u8217', and point out that for the purpose of this article, with treat both terms as synonyms.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\i Although I am not deeply familiar with all work on combining ESM outputs, my understanding is that another common strategy is to reward models that (i) simulate today\u8217's climate well and (ii) remain close to the ensemble consensus for future change. The manuscript cites earlier work (e.g.\u160 ?Tebaldi & Knutti) at several points, but a fuller discussion of how existing methods compare would be valuable. Readers will want guidance on when this weighting scheme should be preferred and why.}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 The literature on multi-model ensamble averages is relatively rich, and there are more approaches than the one mentioned by the reviewer here. It is not our intention to provide here a literature review on this topic, as other reviews already exist. Nevertheless, we added a paragraph in the Discussion section in which we briefly mention the type of available approaches as reviewed by Tebaldi & Knutti (2007), with additions of more recent and relevant references.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\i In addition, I have a minor comments/questions that I hope the authors will be able to address before this is considered for publication.} {\i L95-100 : could the authors expand on why the approximation dismissing K is appropriate? I know this is discussed later in the manuscript as a limitation of the proposed method, but I think it would be useful to also have an argument at this point on why that\u8217's a reasonable approximation to start with.}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We modified this section by first providing a version of equation 5 that includes {\i K}, followed by the approximation version without {\i K}. We explain the reason for not including {\i K} here following the same arguments provided in section 5.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\i L105-110 : \u8220"but given the absence of any other method for obtaining a log-likelihood function of a parameterized ESM with respect to data\u8221" I would recommend nuancing this statement. There exists methods out there that allow to model loglikelihood functions (e.g.\u160 ?variational approaches). This doesn\u8217't diminish the proposed approach, since it might be the simplest first step to take, and in the Occam\u8217's razor philosophy, it makes sense being explored and worthy of a publication.}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We modified this paragraph based on the reviewer\u8217's suggestion. It is true that models that use some parameterization schemes such as the 4D-var method and its variants, provide the possibility to obtain a likelihood function. However, these approaches are often used in one component of the model, and not necessarily to parameterize a fully-coupled ESM. Nevertheless, it may be possible to use results from these optimization approaches to add some information on the likeliihood function for some component of the ESM.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\i Eq 13 : Am I correct in saying that the weights end up being {\i w}{\sub {\i i}}\u8196 ?=\u8196 ?1/{\i \u963 ?}{\sub {\i i}}/\u8721 ?1/{\i \u963 ?}{\sub {\i i}}? I think it would be useful to explicitly include this in the manuscript. The current presentation aims for a greater level of generality in its formalism, which is commendable, and could apply to any choice of distance metric A. However, for the particular choice made by the authors here, the expression of wi simplifies a lot and becomes very interpretable : we simply give more weight to model that have better least square agreement with the observational product.}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We thank the reviewer for this important comment. Indeed, for our particular choice of the metric {\i A}, the weights can also be expressed as the inverse of the variance between model output and observations. In fact, in the literature on maximum likelihood estimation, inverse-variance weighting emerges as an efficient estimate of the mean for populations in which the variances are known and the mean is unknown. This is an interesting connection between the information-theoretic approach and the maximum-likelihood theory, which converges to the formulas of inverse-variance weighting for our choice of metric {\i A}. Based on this result, we modified some of the presentation of the theoretical results, making a better link to the maximum likelihood theory, showing alternative formulas for the weights based on the inverse-variance equations, and presenting a simpler formula for the overall variance. In addition, we found that inverse-variance weighting methods are common in meta-analyses and in the biomedical literature, so we added a few sentences in the Discussion showing these alternative use of the method.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\i Eq 15 : Is this supposed to be a definition of the uncertainty or the variance of {\i x\u772 ?}? If the latter, I don\u8217't understand how it is derived, if the former I would suggest not using {\i x\u772 ?} as a subscript.}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We modified and clarified the representation of variance and uncertainty. Given the previous result on the weights being identical to inverse-variance weighting, we used this result to provide a formal definition of variance. To represent uncertainties, we changed the equations to express them as predictions intervals given that this is a more appropriate way to express the uncertainties for predictions outside the time range where model output and observations overlap.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\i With these points addressed, I believe the paper will make a valuable contribution and be ready for publication.}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\i Just out of curiosity : I appreciate that the distance A is only interpretable as a relative metric. I\u8217've nonetheless always been curious about its interpretation in \u8220"informational units\u8221". What I mean is that Shanon entropy measure information in bits, which can be argued to be an intepretable unit. I guess this doesn\u8217't translate immediately here since in the continuous setting we\u8217're using the differential entropy which is homogeneous to x. But have you thought of ways to make its values as an absolute metric interpretable?}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 The original article of Kullback & Liebler (1951) helps to arrive at an interpretation of our proposed metric {\i A}, and the model differences {\i \u916 ?}. It is important to keep in mind that the origin of the KL divergence was in the context of the statistical problem of discriminating between two populations. Therefore, the interpretation of the KL distance in this context is of the information available to distinguish between two statistical populations (Kullback & Libler 1951, pg. 80). More generally, one can also interpret KL divergence as the information available to distinguish between two probability distributions. This information would be measured in bits or nats depending on the base of the logarithm. Regarding our definition of {\i A} and its interpretation. {\i A} is based on the log-likelihood between model predictions and observations to approximate the KL divergence, so we can interpret {\i A} as the available information to discriminate between the distribution of the model and the observational product. We used the base {\i e} logarithm, so {\i A} is measured in nats. Now, the values of {\i \u916 ?}{\sub {\i i}} are differences among the values {\i A}{\sub {\i i}} for individual models and the values of {\i A} of the model with the lowest divergence to observations. Therefore, we can interpret the values {\i \u916 ?}{\sub {\i i}} as the information available to discriminate between each individual model and the value of the model with the lowest divergence with the observations, measured in units of nats.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\b References}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Kullback, S. and Leibler, R. A.: On Information and Sufficiency, The Annals of Mathematical Statistics, 22, 79\u8211-86, http://www.jstor.org/stable/2236703, 1951.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Tebaldi, C. and Knutti, R.: The use of the multi-model ensemble in probabilistic climate projections, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 365, 2053\u8211-2075, https://doi.org/10.1098/rsta.2007.2076, 2007.\par}
}
